{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data_dantri_chungkhoan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(580, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>580</td>\n",
       "      <td>580</td>\n",
       "      <td>580</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>579</td>\n",
       "      <td>579</td>\n",
       "      <td>315</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>https://dantri.com.vn/kinh-doanh/con-trai-ong-...</td>\n",
       "      <td>Con trai ông Trần Đình Long đã giàu lại giàu thêm</td>\n",
       "      <td>30/10/2023</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "count                                                 580   \n",
       "unique                                                579   \n",
       "top     https://dantri.com.vn/kinh-doanh/con-trai-ong-...   \n",
       "freq                                                    2   \n",
       "\n",
       "                                                    title        time content  \n",
       "count                                                 580         580     580  \n",
       "unique                                                579         315     574  \n",
       "top     Con trai ông Trần Đình Long đã giàu lại giàu thêm  30/10/2023      []  \n",
       "freq                                                    2           6       6  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.describe()\n",
    "\n",
    "data.drop_duplicates(subset=[\"content\"],inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong Tiếng Anh có rất nhiều từ nối và được sử dụng rất thường xuyên như \"and\", \"the\" và \"a\". Khi thực hiện việc thống kê trên văn bản, những từ này sẽ mang lại rất nhiều nhiễu vì chúng xuất hiện thường xuyên hơn các từ khác. Một số pineline về NLP sẽ gắn cờ chúng là các từ dừng (stop words) - nghĩa là các từ mà bạn có thể sẽ muốn lọc ra trước khi thực hiện bất kỳ các phân tích thống kê nào. Tương tự với Tiếng Việt cũng có rất nhiều stop words, chúng ta cần phải loại bỏ chúng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 1942\n",
      "First 10 stopwords: ['a lô', 'a ha', 'ai', 'ai ai', 'ai nấy', 'ai đó', 'alô', 'amen', 'anh', 'anh ấy']\n"
     ]
    }
   ],
   "source": [
    "# Đọc tệp vietnamesestopword.txt và lưu các từ dừng vào một danh sách\n",
    "stopwords = []\n",
    "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "print(f'Number of stopwords: {len(stopwords)}')\n",
    "print(f'First 10 stopwords: {stopwords[:10]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, chúng ta phân tách dữ liệu văn bản của mình thành đơn vị nhỏ nhất được gọi là \"tokens\" hoặc \"words\". Việc phân tích các đoạn văn dài khá khó khăn nên trước tiên chúng ta tách các đoạn văn thành các dòng riêng biệt sau đó các dòng tách thành các từ và xóa stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_para(content):\n",
    "    \"\"\"Làm sạch văn bản bằng cách loại bỏ dấu câu, chuyển đổi thành chữ thường và loại bỏ stopwords.\"\"\"\n",
    "    # Kiểm tra nếu van_ban không phải là chuỗi, thì chuyển thành chuỗi\n",
    "    if not isinstance(content, str):\n",
    "        content = ' '.join(content)\n",
    "    # Chuyển đổi thành chữ thường\n",
    "    content = word_tokenize(content.lower())\n",
    "    # Loại bỏ stopwords\n",
    "    vocab = [lemmatizer.lemmatize(word) for word in content if word not in stopwords]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       content_cleaned\n",
      "0    [bloomberg, billionaire, index, ,, jensen, hua...\n",
      "1    [giao, dịch, giằng, co, phiên, ,, phiên, chiều...\n",
      "2    [kết, phiên, giao, dịch, 22/5, (, mỹ, ), ,, cổ...\n",
      "3    [rung, lắc, phiên, tiếp, tục, chỉnh, đầu, phiê...\n",
      "4    [đỗ, quang, vinh, -, phó, chủ, tịch, hội, đồng...\n",
      "..                                                 ...\n",
      "575  [thị, trường, giao, dịch, căng, thẳng, mệt, mỏ...\n",
      "576  [công, ty, chứng, khoán, agribank, (, agirseco...\n",
      "577  [phiên, hôm, (, 7/3, ), ,, vn-index, tiếp, tục...\n",
      "578  [mặc, lực, chiều, dâng, đà, thu, hẹp, yếu, dần...\n",
      "579  [công, ty, chứng, khoán, agribank, (, agriseco...\n",
      "\n",
      "[580 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "data['content_cleaned'] = data['content'].apply(clean_para)\n",
    "print(data[['content_cleaned']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xây dựng mô hình LSA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Chia dữ liệu thành tập huấn luyện, xác thực và tập kiểm tra 70-15-15\n",
    "text_train, text_val_test = train_test_split(data, test_size=0.3, random_state=42)\n",
    "text_val, text_test = train_test_split(text_val_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Chuyển đổi văn bản thành ma trận TF-IDF\n",
    "vectorizer = TfidfVectorizer(tokenizer=clean_para)\n",
    "train_vectors = vectorizer.fit_transform(text_train['content'])\n",
    "\n",
    "# Áp dụng LSA\n",
    "n_components = 100  # Số chiều của không gian LSA \n",
    "lsa = TruncatedSVD(n_components=n_components)\n",
    "train_lsa = lsa.fit_transform(train_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tóm tắt văn bản***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svd__n_components': 2, 'tfidf__max_df': 0.8, 'tfidf__min_df': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Định nghĩa pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svd', TruncatedSVD()),\n",
    "])\n",
    "\n",
    "# Định nghĩa lưới tham số\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1, 2],  # Try different min_df values\n",
    "    'tfidf__max_df': [0.8, 0.9],  # Try different max_df values\n",
    "    'svd__n_components': [2, 3, 4]  # Start with lower values for n_components\n",
    "}\n",
    "# Tạo đối tượng GridSearchCV. Xác định score.\n",
    "def my_scorer(estimator, X):\n",
    "    X_reduced = estimator.transform(X)\n",
    "    return cosine_similarity(X_reduced).mean()\n",
    "\n",
    "# Thực hiện tìm kiếm lưới\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=my_scorer, verbose=0)\n",
    "grid_search.fit(text_train['content'])\n",
    "\n",
    "# In ra tham số tốt nhất\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, vectorizer, svd_model, train_lsa, n_sentences=3):\n",
    "    text = '\\n'.join([line for line in text.split('\\n') if line.strip()])\n",
    "    text_vector = vectorizer.transform([text])\n",
    "    text_lsa = svd_model.transform(text_vector)\n",
    "    \n",
    "    # Tính toán độ tương đồng cosine giữa các câu và văn bản gốc\n",
    "    similarities = cosine_similarity(text_lsa, train_lsa)[0]\n",
    "    \n",
    "    # Chọn n câu có độ tương đồng cao nhất, đảm bảo không vượt quá số lượng câu có sẵn\n",
    "    num_sentences_to_select = min(n_sentences, len(sent_tokenize(text)))  # Calculate the actual number of sentences to select\n",
    "    top_indices = similarities.argsort()[-num_sentences_to_select:][::-1]  # Select the top indices\n",
    "    top_indices = sorted(top_indices)\n",
    "\n",
    "    # Tạo bản tóm tắt\n",
    "    summary = ' '.join([sent_tokenize(text)[i] for i in top_indices if i < len(sent_tokenize(text))]) # Check if index is valid\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giả sử bộ tham số tốt nhất tìm được là:\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Huấn luyện mô hình với bộ tham số tốt nhất trên tập dữ liệu huấn luyện\n",
    "best_tfidf = TfidfVectorizer(min_df=best_params['tfidf__min_df'], max_df=best_params['tfidf__max_df'])\n",
    "best_svd = TruncatedSVD(n_components=best_params['svd__n_components'])\n",
    "\n",
    "text_train_vector = best_tfidf.fit_transform(text_train['content'])\n",
    "train_data_lsa = best_svd.fit_transform(text_train_vector)\n",
    "\n",
    "# Kiểm tra hiệu suất của mô hình trên tập dữ liệu kiểm định và kiểm tra\n",
    "text_val_vector = best_tfidf.transform(text_val['content'])\n",
    "val_data_lsa = best_svd.transform(text_val_vector)\n",
    "\n",
    "text_test_vector = best_tfidf.transform(text_test['content'])\n",
    "test_data_lsa = best_svd.transform(text_test_vector)\n",
    "\n",
    "# Sử dụng hàm summarize_text để tạo tóm tắt cho các đoạn văn trong tập dữ liệu kiểm định và kiểm tra\n",
    "text_val['summaries'] = [summarize_text(text, best_tfidf, best_svd, train_data_lsa) for text in text_val['content']] \n",
    "text_test['summaries'] = [summarize_text(text, best_tfidf, best_svd, train_data_lsa) for text in text_test['content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296                                                     \n",
       "463                                                     \n",
       "79                                                      \n",
       "543                                                     \n",
       "566                                                     \n",
       "                             ...                        \n",
       "155                                                     \n",
       "441                                                     \n",
       "414                                                     \n",
       "271    Trong khi đó, tốc độ tăng trưởng nợ xấu và tỷ ...\n",
       "84     Tiền đổ vào HoSE đạt 11.661,5 tỷ đồng, khối lư...\n",
       "Name: summaries, Length: 87, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_val['summaries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144    Để được xét duyệt nâng hạng, chứng khoán Việt ...\n",
       "220                                                     \n",
       "553                                                     \n",
       "92                                                      \n",
       "573                                                     \n",
       "                             ...                        \n",
       "423                                                     \n",
       "348                                                     \n",
       "153                                                     \n",
       "551    Tổng số mã giảm trên cả 3 sàn là 523 mã so với...\n",
       "188                                                     \n",
       "Name: summaries, Length: 87, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test['summaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
